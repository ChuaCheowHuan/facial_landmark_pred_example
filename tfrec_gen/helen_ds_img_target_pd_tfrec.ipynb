{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"helen_ds_img_target_pd_tfrec.ipynb","provenance":[],"collapsed_sections":[],"toc_visible":true,"mount_file_id":"1uGwqTW3xbOy6UYIh1Js9dlDKfLRkgWGP","authorship_tag":"ABX9TyMn4c6ji5V6C1AVihgL4UDF"},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","metadata":{"id":"uyS3_W6KPyjh"},"source":["#Combine target and images pandas dataframes."]},{"cell_type":"markdown","metadata":{"id":"ifgB7RWEQF1y"},"source":["##Imports."]},{"cell_type":"code","metadata":{"id":"BVNz2apDQGD6"},"source":["import matplotlib.pyplot as plt\n","from PIL import Image\n","import imageio as iio\n","import io\n","\n","import numpy as np\n","import pandas as pd\n","import tensorflow as tf\n","\n","import time\n","import os\n","import sys"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"7wp79KloQPzx"},"source":["path = '/content/drive/MyDrive/Colab Notebooks/facial_landmark_pred_example/'\n","sys.path.append(path)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"Ae173ErCQGvh"},"source":["##Read target dataframe."]},{"cell_type":"code","metadata":{"id":"-sFAL-uWPys5"},"source":["labels_df = pd.read_pickle(path + \"helen_pickle/\" + \"helen_ds_target_pd.pickle\")\n","\n","labels_df"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"HJ8PxU1dQfiS"},"source":["##Read images dataframes."]},{"cell_type":"code","metadata":{"id":"LbhO5tx9QfpE"},"source":["dfs = []\n","for i in range(5):\n","    names_p_imgs_df = pd.read_pickle(path + \"helen_pickle/\" + \"helen_ds_img_\" + str(i+1) +\".pickle\")\n","    dfs.append(names_p_imgs_df)\n","\n","dfs    "],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"9eQ5B-IgV1BU"},"source":["##Setup tfrecord writer."]},{"cell_type":"code","metadata":{"id":"x2vSLDSuV9wE"},"source":["def test_dataset(dataset):\n","    \"\"\"\n","    Check dataset.\n","    \"\"\"\n","    for ds_img_name, ds_img, ds_coord in dataset.take(3):\n","        print ('{}, {}, {}'.format(type(ds_img_name), type(ds_img), type(ds_coord)))"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"1TV5JxxrV1Ik"},"source":["# The following functions can be used to convert a value to a type compatible\n","# with tf.train.Example.\n","def _bytes_feature(value):\n","  \"\"\"Returns a bytes_list from a string / byte.\"\"\"\n","  if isinstance(value, type(tf.constant(0))):\n","    value = value.numpy() # BytesList won't unpack a string from an EagerTensor.\n","    print(\"_bytes_feature value\", value)\n","    print(\"_bytes_feature tf.train.Feature\", tf.train.Feature(bytes_list=tf.train.BytesList(value=[value])))\n","  return tf.train.Feature(bytes_list=tf.train.BytesList(value=[value]))\n","\n","def _float_feature(value):\n","  \"\"\"Returns a float_list from a float / double.\"\"\"\n","  return tf.train.Feature(float_list=tf.train.FloatList(value=[value]))\n","\n","def _int64_feature(value):\n","  \"\"\"Returns an int64_list from a bool / enum / int / uint.\"\"\"\n","  return tf.train.Feature(int64_list=tf.train.Int64List(value=[value]))\n","\n","\n","def serialize_example(img_name, img, coords):\n","  \"\"\"\n","  Creates a tf.train.Example message ready to be written to a file.\n","  \"\"\"  \n","  # Create a dictionary mapping the feature name to the tf.train.Example-compatible\n","  # data type.\n","  feature = {\n","      'img_name': _bytes_feature(img_name),\n","      'img'     : _bytes_feature(img),\n","      'coords'  : _bytes_feature(coords.numpy().tobytes()),     # Convert np.array of float32 to bytes.\n","  }\n","\n","  # Create a Features message using tf.train.Example.\n","  example_proto = tf.train.Example(features=tf.train.Features(feature=feature))\n","\n","  return example_proto.SerializeToString()\n","\n","def tf_serialize_example(img_name, img, coords):\n","    tf_string = tf.py_function(\n","        serialize_example,\n","        (img_name, img, coords),      # pass these args to the above function.\n","        tf.string)      # the return type is `tf.string`.\n","\n","    return tf.reshape(tf_string, ()) # The result is a scalar\n","\n","def write_tf_rec(dataset, path, i):   \n","    serialized_features_dataset = dataset.map(tf_serialize_example)\n","    print(serialized_features_dataset)\n","\n","    # Write a TFRecord    \n","    #writer = tf.io.TFRecordWriter(path + \"tfrec_data_\" + str(i+1))\n","    writer = tf.data.experimental.TFRecordWriter(path + \"tfrec_data/\" + \"tfrec_data_\" + str(i+1))\n","    writer.write(serialized_features_dataset)    "],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"sLjNXWvuRs_I"},"source":["###Merge dataframes & write tfrecords."]},{"cell_type":"code","metadata":{"id":"h9hRQsBjRtEf"},"source":["for i, df in enumerate(dfs):\n","    # Merge the concatenated names_p_imgs_df with the labels_df.\n","    inner_join_df = pd.merge(left=labels_df, right=df, \n","        left_on=['file_name_label'], right_on=['file_name'], \n","        how='inner')\n","    \n","    inner_join_df = inner_join_df.drop(['file_name_label'], axis=1)\n","\n","    # Select 2nd last column.\n","    img_names = inner_join_df.iloc[:,-2]\n","    # Select last column.\n","    imgs = inner_join_df.iloc[:,-1]    \n","    # Select everything except last 2 columns\n","    coords = inner_join_df.drop(['file_name', 'img'], axis=1)\n","    \n","    # Convert from pandas to tf.data.Dataset.\n","    dataset = tf.data.Dataset.from_tensor_slices((img_names.values, imgs.values, coords.values))\n","\n","    # Write to tfRecord files.\n","    write_tf_rec(dataset, path, i)"],"execution_count":null,"outputs":[]}]}